<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:17px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  h2 {
      font-weight: 300;
      margin-top: 0px; /* Adjust the margin here */
    }

  .disclaimerbox {
    background-color: #eee;
    /* border: 1px solid #eeeeee;*/
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    /* border: 1px solid black; */
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    /* border: 1px solid black; */
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    /*border: 1px solid #eeeeee; */
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }
    

    .column2 {
      float: left;
      width: 40%;
      padding: 15px;
    }
    
    .column {
      float: left;
      width: 32%;
      padding: 3px;
    }

    /* Clear floats after image containers */
    .row::after {
      content: "";
      clear: both;
      display: table;
    }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }
    img{
        border-width: 0px;
    }
    

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
    .normaltext-cfl{
        width:800px; 
        margin:0 auto; 
        text-align:justify; 
        font-size: 15px;
    }
    .table-images tr{
        width:800px; 
        margin:0 auto; 
    }
    .table-images tr td{
        white-space: nowrap;
        width:"100%"
    }
    .table-images tr td img{
        max-width:100%; 
  max-height:100%;
  margin:auto;
  display:block;
    }
</style>
<!-- Start : Google Analytics Code -->
<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <head>
    
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JGJP9W0E2J"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-JGJP9W0E2J');
  </script>
    
    <title>The Drunkard’s Odometry: Estimating Camera Motion in Deforming Scenes</title>
    <meta property='og:title' content='The Drunkard’s Odometry: Estimating Camera Motion in Deforming Scenes' />
    <meta property="og:description" content="The Drunkard’s Odometry: Estimating Camera Motion in Deforming Scenes" />
  </head>

  <body>     
        <br>
        <center><span style="font-size:33px;font-weight:bold;">The Drunkard’s Odometry: Estimating Camera Motion in Deforming Scenes</span></center><br/>
        <center><h2>[NeurIPS 2023]</h2></center>
        <table align=center width=600px>
          <tr>
            <td align=center width=50px>
            <center><span style="font-size:16px"><a href="https://davidrecasens.github.io/" target="_blank">David Recasens<sup>1</sup></a></span></center></td>
            <td align=center width=50px>
            <center><span style="font-size:16px"><a href="http://people.inf.ethz.ch/moswald/" target="_blank">Martin R. Oswald<sup>2,3</sup></a></span></center></td>
            <td align=center width=50px>
            <center><span style="font-size:16px"><a href="https://people.inf.ethz.ch/pomarc/" target="_blank">Marc Pollefeys<sup>2,4</sup></a></span></center></td>
            <td align=center width=50px> 
            <center><span style="font-size:16px"><a href="https://janovas.unizar.es/sideral/CV/javier-civera-sancho" target="_blank">Javier Civera<sup>1</sup></a></span></center></td> 
          </tr>
         </table>
         
        <table align=center width=600px style="padding-top:0px;padding-bottom:20px">
          <tr>            
            <td align=center width=100px>
            <center><span style="font-size:15px"> <sup>1</sup>University of Zaragoza</span></center></td> 
            <td align=center width=100px>
            <center><span style="font-size:15px"> <sup>2</sup>ETH Zurich</span></center></td> 
            <td align=center width=100px>
            <center><span style="font-size:15px"> <sup>3</sup>University of Amsterdam</span></center></td>  
            <td align=center width=100px>
            <center><span style="font-size:15px"> <sup>4</sup>Microsoft</span></center></td>          
          </tr>          
        </table>
        <!--<table align=center width=600px>
          <tr>
            <td align=center width=650px>
            <center><span style="font-size:22px"><a href="" target="_blank"> --- </a></span></center></td>
          </tr>
        </table>-->
        <table align=center width=600px>
          <tr>                       
            <td align=center width=250px><center><span style="font-size:17px"><a href='#download-dataset'>[The Drunkard's Dataset]</a></span></center></td>
            <td align=center width=250px><center><span style="font-size:17px"><a href='#sourceCode'>[The Drunkard's Odometry]</a></span></center></td>
            <td align=center width=250px><center><span style="font-size:17px"><a href='#paper'>[Preprint]</a></span></center></td>
            </tr>
        </table>
        <br>

        
        <div class="normaltext-cfl" style="">
          
        <table align=center>
       
        <tr>
          <td>
            <table align=center width=320>
            <tr><td align=center width=320>
            
            <div style="position: relative; width: 560px; height: 315px;">
               <iframe width="560" height="315" src="https://www.youtube.com/embed/wL8JDg6bemg?autoplay=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
               <img src="images/thumbnail_low.jpg" alt="Fallback Image" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; display: none;">
            </div>            
            </td></tr>
            </table>
          </td>
          </table>
          
          
    
          <br>   
          <style>
             .narrow-paragraph {
             width: 73%;
             margin: 0 auto;
             display: flex;
             justify-content: center;
             }
          </style>  
          <div class="narrow-paragraph">
          <i>Sample scene of the Drunkard’s Dataset. The dataset provides various levels of scene deformation. Top row: Sample frames from scene 0 over all difficulty levels 0 − 3. Bottom row: External views showing the ground truth camera trajectory in green and the camera frame in purple. With increasing deformation level the camera motion is more abrupt.</i>
          </div>
          <br>          
          
          
        <br>

        <div id="abstract" class="normaltext-cfl" style="">
          Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the <i>Drunkard's Dataset</i>, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the <i>Drunkard's Odometry</i>, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data.
                  <br><br>

      </div>


      <hr>

        <center id="download-dataset"><h1>The Drunkard's Dataset</h1></center>
        <div class="normaltext-cfl" style="">
        
        <table align=center width=1000px>
          <tr><td width=100px>
            <center><a href="images/samples_datasets.jpg" style="margin-right:1px;margin-left: -200px"><img src = "images/samples_datasets.jpg" height="400px" border="1" style="border-color:black"></a>
            <br></center>
          </td></tr>
        </table>   
        
        <table align=center width=800px>
        <tr>
          <!-- <p style="margin-top:4px;"></p> -->
          <td width=300px align=center>
            <span style="font-size:18px"><a href='https://drive.google.com/drive/folders/1AZHUKMbe7bR1xwRmAAZ0AHgcEqRnNjms?usp=sharing'>[Download Dataset]</a></span>          
          </td>
        </table>

        <br>
          In the link above you can download The Drunkard's Dataset.The root folder contains two similar versions of the dataset but with different image resolutions, 1024x1024 and 320x320 pixels. Both versions have the same folder structure as follows:        
        <br> 
<br>


<head>
  <style>
    /* Center the ul element */
    ul {
      margin: 0 auto;
      text-align: left;
      width: fit-content;
    }

    /* Use small circles as list-style type */
    ul, li {
      list-style-type: circle;
    }

    /* Use smaller circles */
    ul ul, ul ul li {
      list-style-type: circle;
    }

    /* Use even smaller circles */
    ul ul ul, ul ul ul li {
      list-style-type: circle;
    }
  </style>
</head>
<body>
  <ul>
    <li>
      <strong>The Drunkard's Dataset</strong>
         <ul>
           <li>
             <strong>Dataset resolutions</strong>
      <ul>
        <li>
          <strong>Scenes</strong>
          <ul>
            <li>
              <strong>Difficulty levels</strong>
              <ul>
                <li><strong>Color</strong></li>
                <li><strong>Depth</strong></li>
                <li><strong>Optical flow</strong></li>
                <li><strong>Normal</strong></li>
                <li><strong>Pose</strong></li>
              </ul>
            </li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</li>
  </ul>

</body>

<br>

For every of the 19 scenes there are 4 levels of deformation difficulty and inside each of them you can find color and depth images, optical flow and normal maps and the camera trajectory.

<p>- <strong>Color</strong>: RGB uint8 .png images.</p>
<p>- <strong>Depth</strong>: uint16 .png grayscale images whose pixel values must be multiplied by (2 ** 16 - 1) * 30 to obtain metric scale in meters.</p>
<p>- <strong>Optical flow</strong>: .npy image numpy arrays that are .npz compressed. They have two channels: horizontal and vertical pixel translation to go from current frame to the next one.</p>
<p>- <strong>Normal</strong>: .npy image numpy arrays that are .npz compressed. There are three channels: x, y and z to represent the normal vector to the surface where the pixel falls.</p>
<p>- <strong>Camera trajectory pose</strong>: .txt file containing at each line a different SE(3) world-to-camera transformation for every frame. Format: timestamps, translation (tx, ty, tz), quaternions (qx, qy, qz, qw).</p> 

Check the <a href="https://github.com/UZ-SLAMLab/DrunkardsOdometry/blob/5e2eb3fa630ca25dc003a3873ed11b2068ff3893/data_readers/drunkards.py" target="_blank">Drunkard's Odometry dataloader</a> for further coding technical details to work with the data.
           <br>            
      <br>
      <hr>   
   

        <center id="sourceCode"><h1>The Drunkard's Odometry</h1></center>

         <div class="normaltext-cfl" style="">         
         
         <table align=center width=1000px>
          <tr><td width=100px>
            <center><a href="images/Overview_drunk2.jpg" style="margin-right:1px;margin-left: -200px"><img src = "images/Overview_drunk2.jpg" height="400px" border="1" style="border-color:black"></a>
            <br></center>
          </td></tr>
         </table>
           
   
             
        <table align=center width=900px>
        <tr>
          <!-- <p style="margin-top:4px;"></p> -->
          <td width=300px align=center>
            <span style="font-size:18px; margin-left: -120px"><a href='https://github.com/UZ-SLAMLab/DrunkardsOdometry'>[GitHub]</a></span>
          </td>
        </tr>
        </table>

        <br>    
        
        The source code of the Drunkard's Odometry is available to use at GitHub in the link above. There you will find a detailed explanation to execute training and evaluations scripts.       
        <br>
          
        </div>
       <br>
      <hr>
          
          <center id="paper"><h1>Paper and Bibtex</h1></center>
           <table align=center width=550px>
       <!-- <div class="paper" id="cfl19_bib" align="center" width = "550px">-->
        <tr>
            <td width=550px>
<pre  xml:space="preserve">

arXiv paper available <a href='https://arxiv.org/abs/2306.16917'>here</a>.

@article{recasens2023drunkard,
  title={The Drunkard's Odometry: Estimating Camera Motion in Deforming Scenes},
  author={Recasens, David and Oswald, Martin R and Pollefeys, Marc and Civera, Javier},
  journal={arXiv preprint arXiv:2306.16917},
  year={2023}
}

    <hr>
          
        <center id="funding"><h1>Funding</h1></center>
         <table align=center width=550px>
     <!-- <div class="paper" id="cfl19_bib" align="center" width = "550px">-->
      <tr>
          <td width=550px>
<pre  xml:space="preserve">

This work was supported by the EU Comission (EU-H2020 EndoMapper GA 863146), the Spanish Government 
(PID2021-127685NB-I00 and TED2021-131150BI00), the Aragon Government (DGA-T45 17R/FSE), and a 
research grant from FIFA.


    <hr>
          
        <center id="license"><h1>License</h1></center>
         <table align=center width=550px>
      <tr>
          <td width=550px>
<pre  xml:space="preserve">

The code, dataset and additional resources of this work are released under <a href='https://github.com/UZ-SLAMLab/DrunkardsOdometry/blob/main/LICENSE'>MIT License</a>. 
There are some parts of the code modified from other repositories subject also to their own license. 
Check the GitHub repository for further details.


    <br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<p style="text-align:right;font-size:small;">                
  <br>
    Based on the <a href="https://cfernandezlab.github.io/CFL/">Clara Fernandez</a>'s template.
  </p>
</body>
</html>
