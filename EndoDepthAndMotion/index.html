<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:17px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    /* border: 1px solid #eeeeee;*/
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    /* border: 1px solid black; */
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    /* border: 1px solid black; */
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    /*border: 1px solid #eeeeee; */
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }
    

    .column2 {
      float: left;
      width: 40%;
      padding: 15px;
    }
    
    .column {
      float: left;
      width: 32%;
      padding: 3px;
    }

    /* Clear floats after image containers */
    .row::after {
      content: "";
      clear: both;
      display: table;
    }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }
    img{
        border-width: 0px;
    }
    

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
    .normaltext-cfl{
        width:800px; 
        margin:0 auto; 
        text-align:justify; 
        font-size: 15px;
    }
    .table-images tr{
        width:800px; 
        margin:0 auto; 
    }
    .table-images tr td{
        white-space: nowrap;
        width:"100%"
    }
    .table-images tr td img{
        max-width:100%; 
  max-height:100%;
  margin:auto;
  display:block;
    }
</style>
<!-- Start : Google Analytics Code -->
<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <head>
    
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-P9C5NDJ');</script>
    <!-- End Google Tag Manager -->
    
    <title>Endo-Depth-and-Motion: Localization and Reconstruction in Endoscopic Videos using Depth Networks and Photometric Constraints</title>
    <meta property='og:title' content='Endo-Depth-and-Motion: Localization and Reconstruction in Endoscopic Videos using Depth Networks and Photometric Constraints' />
    <meta property="og:description" content="Endo-Depth-and-Motion: Localization and Reconstruction in Endoscopic Videos using Depth Networks and Photometric Constraints" />
  </head>

  <body>
        <!-- Google Tag Manager (noscript) -->
        <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P9C5NDJ"
        height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
        <!-- End Google Tag Manager (noscript) -->
    
        <br>
        <center><span style="font-size:33px;font-weight:bold;">Endo-Depth-and-Motion: Localization and Reconstruction in Endoscopic Videos using Depth Networks and Photometric Constraints</span></center><br/>
        <table align=center width=1000px>
          <tr>
            <td align=center width=260px>
            <center><span style="font-size:16px"><a href="https://davidrecasens.github.io/" target="_blank">David Recasens</a></span></center></td>
            <td align=center width=150px>
            <center><span style="font-size:16px"><a href="https://webdiis.unizar.es/~jlamarca/" target="_blank">José Lamarca</a></span></center></td>
            <td align=center width=220px>
            <center><span style="font-size:16px"><a href="https://webdiis.unizar.es/~jmfacil/" target="_blank">José M. Fácil</span></center></td>
            <td align=center width=200px>
            <center><span style="font-size:16px"><a href="http://webdiis.unizar.es/~josemari/" target="_blank">J. M. M. Montiel</a></span></center></td>
            <td align=center width=150px>            
            <center><span style="font-size:16px"><a href="https://janovas.unizar.es/sideral/CV/javier-civera-sancho" target="_blank">Javier Civera</a></span></center></td> 
          </tr>
         </table>
         
        <table align=center width=600px style="padding-top:0px;padding-bottom:20px">
          <tr>            
            <td align=center width=600px><center><span style="font-size:15px"> University of Zaragoza</span></center></td>            
          </tr>          
        </table>
        <!--<table align=center width=600px>
          <tr>
            <td align=center width=650px>
            <center><span style="font-size:22px"><a href="" target="_blank"> --- </a></span></center></td>
          </tr>
        </table>-->
        <table align=center width=250px>
          <tr>
            <td align=center width=100px><center><span style="font-size:17px"><a href="">[Paper (soon)]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:17px"><a href='#download-dataset'>[Dataset]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:17px"><a href='#sourceCode'>[Code]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:17px"><a href='#videos'>[Videos]</a></span></center></td>
            </tr>
        </table>
        <br>

        <table align=center width=1000px>
          <tr><td width=100px>
            <center><a href="images/teaser_figure2.png" style="margin-right:1px;margin-left: 55px"><img src = "images/teaser_figure2.png" height="349px" border="1" style="border-color:black"></a>
          </center>
          </td></tr>
        </table>
        <br>

        <div id="abstract" class="normaltext-cfl" style="">
          Estimating a scene reconstruction and the camera motion from in-body videos is challenging due to several factors, <i>e.g.</i> the deformation of in-body cavities or the lack of texture. In this paper we present <i>Endo-Depth-and-Motion</i>, a pipeline that estimates the 6-degrees-of-freedom camera pose and dense 3D scene models from monocular endoscopic videos. Our approach leverages recent advances in self-supervised depth networks to generate <i>pseudo</i>-RGBD frames, then tracks the camera pose using photometric residuals and fuses the registered depth maps in a volumetric representation. We present an extensive experimental evaluation in the public dataset Hamlyn, showing high-quality results and comparisons against relevant baselines.
        <br><br>

      </div>


      <hr>

        <center id="download-dataset"><h1>Hamlyn rectified Dataset with ground truth, Endo-Depth Models and Splits</h1></center>
        <div class="normaltext-cfl" style="">
        
        <br>
          Here you can download the rectified stereo images, the calibration and the ground truth of the in vivo endoscopy stereo video dataset of the <a href="http://hamlyn.doc.ic.ac.uk/vision/" target="_blank">Hamlyn Center Laparoscopic</a> at Imperial College London. The ground truth has been created with the stereo matching software <a href="http://www.cvlibs.net/software/libelas/" target="_blank">Libelas</a>. The rectified color images are stored as uint8 .jpg RGB images and the depth maps in mm as uint16 .png.
          Also, you are free to download the Endo-Depth depth prediction models trained on our rectified Hamlyn dataset with monocular, stereo and monocular plus stereo losses, and the used splits. The models will be available soon.
        <br><br>
        
        
        <table align=center width=800px>
        <tr>
          <!-- <p style="margin-top:4px;"></p> -->
          <td width=300px align=center>
            <span style="font-size:18px"><a href='https://drive.google.com/drive/folders/1SYRByyAdlySvltn0CFQea1UY3AoutnKu'>[Download Dataset]</a></span>
          <td width=300px align=center>
            <span style="font-size:18px"><a href='https://drive.google.com/drive/folders/17t30Jz3X-BSz-Fz7BkONqRQsOOaf5xR9'>[Download Models]</a></span>
          <td width=300px align=center>
            <span style="font-size:18px"><a href='https://drive.google.com/drive/folders/1m6FwsBWI2-BDBo-uBP4RQlYbSlAGRQv3?usp=sharing'>[Download Splits]</a></span>
          </td>
        </table>
        
        <br>       
        
         <table align=center width=1000px>
          <tr><td width=100px>
            <center><a href="images/depths.png" style="margin-right:1px;margin-left: -200px"><img src = "images/depths.png" height="159px" border="1" style="border-color:black"></a>
            <br></center>
          </td></tr>
        </table>
                    
      <br>
      <hr>      

        <center id="sourceCode"><h1>Source Code</h1></center>

         <div class="normaltext-cfl" style="">
         
         <!--
         <table align=center width=1000px>
          <tr><td width=100px>
            <center><a href="../imgs/kernels.png" style="margin-right:1px;margin-left: -200px"><img src = "../imgs/equic.png" height="200px" border="1" style="border-color:black"></a>
            <br></center>
          </td></tr>
        </table>
        -->
             
        <br>     
        The source code of the photometric tracking and the volumetric reconstruction will be available to free use soon.
        <br><br>
             
        <table align=center width=900px>
        <tr>
          <!-- <p style="margin-top:4px;"></p> -->
          <td width=300px align=center>
            <span style="font-size:18px; margin-left: -120px"><a href=''>[GitHub]</a></span>
          </td>
        </tr>
        </table>

        <br><br>
        
        <!--
        <table align=center width=1000px>
          <tr><td width=100px>
            <center><a href="../imgs/kernels.png" style="margin-right:1px;margin-left: -200px"><img src = "../imgs/equi_convs.png" height="400px" border="1" style="border-color:black"></a>
            <br></center>
          </td></tr>
        </table>
        -->
             
       </div>
      <br>
      <hr>
    
        <center id="videos"><h1>Videos</h1></center>
        <div class="normaltext-cfl" style="">
          
        <table align=center>
        <tr>
          <center><h3 style="color:dodgerblue">Endo-Depth-and-Motion: Localization and Reconstruction in Endoscopic Videos</h3></center>
        </tr>
        <tr>
          <td>
            <table align=center width=320>
            <tr><td align=center width=320>
            <iframe width="560" height="315" src="https://youtube.com/embed/G1XWIyEbvPc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            
            </td></tr>
            </table>
          </td>
          </table>
    
          <br>     
          <i>Endo-Depth-and-Motion's photometric tracking and volumetric reconstruction working on several Hamlyn sequences. 
          <br>          
          <br>
            
        <table align=center>
        <tr>
          <center><h3 style="color:dodgerblue">Depth estimation of Endo-Depth and Libelas on Hamlyn dataset</h3></center>
        </tr>
        <tr>
          <td>
            <table align=center width=320>
            <tr><td align=center width=320>
            <iframe width="560" height="315" src="https://youtube.com/embed/V3Be2W3iomI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            
            </td></tr>
            </table>
          </td>
          </table>
    
          <br>     
          Depth estimation using the stereo matching method Libelas and the unsupervised deep learning algorithm Endo-Depth (based on Monodepth2), which was trained using stereo mode with a first layer model resolution close to the input images. The depth estimation of Libelas is used as ground truth for Endo-Depth evaluation. The point clouds are made with the depth estimation of Endo-Depth. 
          Libelas and Monodepth2 estimations have pixel color consistency between frames of the same instant. 
<br> 
          
        <br>
          
        </div>
       <br>
      <hr>
  
          <center><h1>Paper and Bibtex</h1></center>
           <table align=center width=550px>
       <!-- <div class="paper" id="cfl19_bib" align="center" width = "550px">-->
        <tr>
            <td width=550px>
<pre  xml:space="preserve">
@article{
bla bla bla
}</pre>
                </td>
</tr>           
        
        </table>

      <br>




    <br><br>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<p style="text-align:right;font-size:small;">                
  <br>
    Based on the <a href="https://cfernandezlab.github.io/CFL/">Clara Fernandez</a>'s template.
  </p>
</body>
</html>
